# Research Findings Motivations
- Language models perform worse with Yes/No type questions?
- Prompts which have high perplexity for the model perform better
- 

# Methods to Test Out
GPTJ-6 
- majority vote over 5 handcrafted prompts with yes/No output (extend prompt_handcrafted_code)
- majority vote over 5 handcrafted prompts with context and yes/No output
- majority vote over 5 open-ended questions and parsing out of output to Yes/No
- majority vote over 5 open-ended questions with context and parsing of output to Yes/No output
- Majority vote w/ Pile Dataset method and output parsing
- AMA method 

FineTunedModel
- Pile Dataset forma github style with Q:\n\n and A:\n\n respectively. This is how Github questions and answers are presented to language model 

# Next Steps - Use this method with GPT3 full size / 