{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the GPT models on the SPOT Dataset\n",
    "# Currently using the Chat.OpenAI.GPT3.5/4  Engine\n",
    "\n",
    "## This requires manually copying questions into the browser and waiting for response\n",
    "## This has also been implemented using the openAI.API, however this charges on a per token basis. OpenAI provide a free starting $20 budget. I used it prior to the significant price decrease and exhausted my budget.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the experiment output from one of the EleutherAI GPT-Neo models to generate the list of questions to pass into the Chat.OpenAI interface\n",
    "# We use the version of Eleuther that processes Yes/No questions with zero-shot context\n",
    "import pandas as pd\n",
    "import ujson as json\n",
    "import pprint\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4, width=260)\n",
    "experiment_output = pd.read_csv('eleuther_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = open('./openai.key.temp','r').read()\n",
    "\n",
    "from util_openai import OpenAICompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Questions to Pass to Chat.OpenAI\n",
    "preds_prompts = experiment_output['preds_prompts'].tolist()\n",
    "preds_prompts = [json.decode(x) for x in preds_prompts]\n",
    "\n",
    "# Remove the text 'Question:' from the start of each prompt\n",
    "preds_prompts = [ x[0].replace('Question: ', '') for x in preds_prompts]\n",
    "\n",
    "# Remove double qoutation marks in each prompt\n",
    "preds_prompts = [ x.replace('\"\"', '\"') for x in preds_prompts]\n",
    "\n",
    "# Remove the text 'Answer:' from the start of each prompt\n",
    "preds_prompts_direct = [x.replace('\\nAnswer:', '') for x in preds_prompts]\n",
    "preds_prompts_indirect = [x.replace('directly', 'indirectly') for x in preds_prompts_direct]\n",
    "preds_prompts_normal = [x.replace('directly ', '') for x in preds_prompts_direct]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "oac = OpenAICompletion(\n",
    "    openai_model='gpt-3.5-turbo',\n",
    "    max_tokens=5,\n",
    "    temperature=1.0,\n",
    "    system_start='Assistant is an intelligent chatbot designed to help users answers questions about whether government spending on \"budget item\" affects a \"socio-economic indicator\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 410/725 [19:50<14:29,  2.76s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit reached for default-gpt-3.5-turbo in organization org-fPO4EPPHMh44ZiLPLQMVeppD on requests per min. Limit: 20 / min. Please try again in 3s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 725/725 [36:04<00:00,  2.99s/it]  \n",
      " 34%|███▍      | 246/725 [12:01<22:01,  2.76s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit reached for default-gpt-3.5-turbo in organization org-fPO4EPPHMh44ZiLPLQMVeppD on requests per min. Limit: 20 / min. Please try again in 3s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 716/725 [35:46<00:24,  2.77s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 725/725 [41:23<00:00,  3.43s/it]\n",
      "100%|██████████| 725/725 [35:35<00:00,  2.95s/it]  \n"
     ]
    }
   ],
   "source": [
    "responses_direct = oac.get_responses(preds_prompts_direct)\n",
    "responses_indirect = oac.get_responses(preds_prompts_indirect)\n",
    "responses_normal = oac.get_responses(preds_prompts_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Responses to Yes/No/Nan\n",
    "def convert_to_yes_no_nan(response):\n",
    "    if 'no' in response.lower():\n",
    "        return 'No'\n",
    "    if 'yes' in response.lower():    \n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'NA'\n",
    "\n",
    "responses_direct = [convert_to_yes_no_nan(x) for x in responses_direct]\n",
    "responses_indirect = [convert_to_yes_no_nan(x) for x in responses_indirect]\n",
    "responses_normal = [convert_to_yes_no_nan(x) for x in responses_normal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# Save Predictions to File\n",
    "# Drop the following columns from experiment_output, preds_ensemble_parsed, preds_ensemble, preds_prompts\n",
    "experiment_output = experiment_output.drop(columns=['preds_ensemble_parsed', 'preds_ensemble', 'preds_prompts','preds_aggregate'], axis=1, errors='ignore')\n",
    "\n",
    "# Setting up the experiment_output dataframe to save the predictions\n",
    "experiment_output_direct = copy.deepcopy(experiment_output)\n",
    "experiment_output_indirect = copy.deepcopy(experiment_output)\n",
    "experiment_output_normal = copy.deepcopy(experiment_output)\n",
    "\n",
    "# Add the predictions to the experiment_output dataframes\n",
    "experiment_output_direct['preds_aggregate'] = responses_direct\n",
    "experiment_output_direct['preds_prompts'] = [json.encode(x) for x in preds_prompts_direct]\n",
    "\n",
    "experiment_output_indirect['preds_aggregate'] = responses_indirect\n",
    "experiment_output_indirect['preds_prompts'] = [json.encode(x) for x in preds_prompts_indirect]\n",
    "\n",
    "experiment_output_normal['preds_aggregate'] = responses_normal\n",
    "experiment_output_normal['preds_prompts'] = [json.encode(x) for x in preds_prompts_normal]\n",
    "\n",
    "# Create the config files\n",
    "config_direct = {'dset_name':'spot', 'k_shot':0, 'nn_name':'gpt-3.5-turbo', 'directly_or_indirectly':'directly'}\n",
    "config_indirect = {'dset_name':'spot', 'k_shot':0, 'nn_name':'gpt-3.5-turbo', 'directly_or_indirectly':'indirectly'}\n",
    "config_normal = {'dset_name':'spot', 'k_shot':0, 'nn_name':'gpt-3.5-turbo', 'directly_or_indirectly':'none'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating file directories\n",
    "dir_ = '../output'\n",
    "\n",
    "dir_direct = 'spot_gpt35turbo_directly'\n",
    "dir_indirect = 'spot_gpt35turbo_indirectly'\n",
    "dir_normal = 'spot_gpt35turbo'\n",
    "\n",
    "os.makedirs(os.path.join(dir_, dir_direct), exist_ok=True)\n",
    "os.makedirs(os.path.join(dir_, dir_indirect), exist_ok=True)\n",
    "os.makedirs(os.path.join(dir_, dir_normal), exist_ok=True)\n",
    "\n",
    "# Saving the experiment_output dataframes \n",
    "experiment_output_direct.to_csv(os.path.join(dir_, dir_direct, 'predictions.csv'),index=False)\n",
    "experiment_output_indirect.to_csv(os.path.join(dir_, dir_indirect, 'predictions.csv'),index=False)\n",
    "experiment_output_normal.to_csv(os.path.join(dir_, dir_normal, 'predictions.csv'),index=False)\n",
    "\n",
    "# Saving the config files\n",
    "with open(os.path.join(dir_, dir_direct, 'config.yaml'), 'w') as f:\n",
    "    yaml.dump(config_direct, f)\n",
    "    \n",
    "with open(os.path.join(dir_, dir_indirect, 'config.yaml'), 'w') as f:\n",
    "    yaml.dump(config_indirect, f)\n",
    "    \n",
    "with open(os.path.join(dir_, dir_normal, 'config.yaml'), 'w') as f:\n",
    "    yaml.dump(config_normal, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alanturing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
