{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "os.environ['HF_HOME'] = '/mnt/Data1/akann1w0w1ck/AlanTuring/.cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/mnt/Data1/akann1w0w1ck/AlanTuring/.cache/transformers'\n",
    "\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from argparse import Namespace\n",
    "\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "import glob\n",
    "from torch.utils.data import DataLoader, Dataset as TorchDataset\n",
    "import yaml\n",
    "from datasets import Dataset  # type: ignore\n",
    "import torch\n",
    "from argparse import ArgumentParser\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "import lightning.pytorch as pl\n",
    "import transformers\n",
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.CRITICAL)\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from peft import get_peft_config, prepare_model_for_int8_training, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from datasets import interleave_datasets, load_dataset\n",
    "\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import gc\n",
    "model_id = 'TheBloke/Wizard-Vicuna-13B-Uncensored-HF'\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "map_modelid_targetmodule = {\n",
    "    'TheBloke/Wizard-Vicuna-7B-Uncensored-HF': ['k_proj', 'v_proj'],\n",
    "    'TheBloke/Wizard-Vicuna-13B-Uncensored-HF': ['k_proj', 'v_proj']\n",
    "}\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:18<00:00,  6.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6553600 || all params: 6678533120 || trainable%: 0.09812933292752765\n"
     ]
    }
   ],
   "source": [
    "# Load Model\n",
    "# Creating Model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "model_id = 'TheBloke/Wizard-Vicuna-13B-Uncensored-HF'\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                        trust_remote_code=True,\n",
    "                                                        quantization_config=bnb_config,\n",
    "                                                        # device_map={'':0},  \n",
    "                                                            device_map = 'auto'\n",
    "                                                        )\n",
    "\n",
    "# Implementing Lora version\n",
    "peft_config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    target_modules=map_modelid_targetmodule[model_id ], \n",
    "    lora_dropout=0, \n",
    "    bias=\"none\", \n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "# prepare int-8 model for training\n",
    "# model = prepare_model_for_int8_training(model)\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# Creating Tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id, use_fast=True, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset for preprocessed research papers\n",
    "dir_data = '../../data'\n",
    "dataset_rp = Dataset.load_from_disk(os.path.join(\n",
    "                dir_data, 'researchpapers', 'preprocessed' ,f\"rp_{model_id.replace('/','_')}_train.arrow\"))\n",
    "dataloader_rp = DataLoader(dataset_rp, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset for instruct to finetune\n",
    "dir_data = '../../data'\n",
    "dataset_ft = Dataset.load_from_disk(os.path.join(\n",
    "                dir_data, 'instruct', 'preprocessed' , f\"wLM70k_nofilt_{model_id.replace('/','_')}_train.arrow\"))\n",
    "dataloader_ft = DataLoader(dataset_ft, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11888"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43979"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function which gets loss and evaluate with different settings\n",
    "\n",
    "def get_loss(llm, batch ):\n",
    "    \n",
    "    outputs = llm(**batch, output_hidden_states=False, output_attentions=False)\n",
    "    loss = outputs.loss\n",
    "\n",
    "    # if torch.isnan(loss):\n",
    "    #     return None\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating RP loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.5469, dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of outputs from dataset_rp\n",
    "batch_rp = next(iter(dataloader_rp))\n",
    "loss_rp = get_loss(model, batch_rp)\n",
    "print(loss_rp)\n",
    "\n",
    "# Move the tensors to CPU\n",
    "batch_rp = {key: value.cpu() for key, value in batch_rp.items() if torch.is_tensor(value)}\n",
    "if torch.is_tensor(loss_rp):\n",
    "    loss_rp = loss_rp.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7188, dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Testing w/ labels tag changed\n",
    "# Get a batch of outputs from dataset_rp\n",
    "batch_rp = next(iter(dataloader_rp))\n",
    "\n",
    "\n",
    "batch_rp[\"labels\"] = batch_rp[\"input_ids\"].clone()\n",
    "\n",
    "\n",
    "loss_rp = get_loss(model, batch_rp)\n",
    "print(loss_rp)\n",
    "\n",
    "# Move the tensors to CPU\n",
    "batch_rp = {key: value.cpu() for key, value in batch_rp.items() if torch.is_tensor(value)}\n",
    "if torch.is_tensor(loss_rp):\n",
    "    loss_rp = loss_rp.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7188, dtype=torch.float16, grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the variables\n",
    "try:\n",
    "    del batch_rp\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "try:\n",
    "    del loss_rp\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "gc.collect()\n",
    "\n",
    "# Clear the GPU cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating instruct loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ft = next(iter(dataloader_ft))\n",
    "batch_ft['labels'] = batch_ft['input_ids'].clone()\n",
    "loss_ft = get_loss(model, batch_ft)\n",
    "loss_ft\n",
    "\n",
    "# Move the tensors to CPU\n",
    "batch_ft = {key: value.cpu() for key, value in batch_ft.items() if torch.is_tensor(value)}\n",
    "if torch.is_tensor(loss_ft):\n",
    "    loss_ft = loss_ft.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1777, dtype=torch.float16, grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the variables\n",
    "try:\n",
    "    del batch_ft\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "try:\n",
    "    del loss_ft\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "gc.collect()\n",
    "\n",
    "# Clear the GPU cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alanturing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
