{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akann1warw1ck/miniconda3/envs/alanturing/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os , sys\n",
    "# set following environment variables\n",
    "os.environ['HF_HOME'] = \"/mnt/Data1/akann1warw1ck/AlanTuring/.cache\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"/mnt/Data1/akann1warw1ck/.cache/transformers\"\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "sys.path.append('/home/akann1warw1ck/AlanTuring')\n",
    "from prompt_engineering.langchain.utils import load_llm\n",
    "from prompt_engineering.langchain.utils import PredictionGenerator\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "# Filter out the warning message\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='transformers')\n",
    "\n",
    "#Testing loading large model on 2 GPUS\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from langchain import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## practicing loading on multiple GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 29/29 [26:31<00:00, 54.87s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "from accelerate import infer_auto_device_map\n",
    "\n",
    "# model_id = 'upstage/llama-30b-instruct-2048'\n",
    "model_id = 'stabilityai/StableBeluga2'\n",
    "# model_id = 'stabilityai/StableBeluga-7B'\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=False,\n",
    "    llm_int8_has_fp16_weights=False,\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\" ,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if True else None\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# device_map = infer_auto_device_map(my_model, max_memory={0: \"10GiB\", 1: \"10GiB\", \"cpu\": \"30GiB\"})\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"balanced\",\n",
    "    max_memory = {0:'22GiB', 1:'22GiB'},\n",
    "    quantization_config=quant_config,\n",
    "    trust_remote_code=True,\n",
    "    # rope_scaling={\"type\": \"dynamic\", \"factor\": 2} # allows handling of longer inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   835,  4911, 29901,    13,  1349, 18902,   338,  9045, 29891,\n",
       "         29889,  1724,  1033,   367,   278,  9590, 29973,    13,    13,  2277,\n",
       "         29937,  4007, 22137, 29901,    13,  1670,  1033,   367]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"### User:\\nThomas is healthy. What could be the reasons?\\n\\n### Assistant:\\n\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs,  max_new_tokens=3)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.87s/it]\n",
      "Device has 2 GPUs available. Provide device={deviceId} to `from_model_id` to use availableGPUs for execution. deviceId is -1 (default) for CPU and can be a positive integer associated with CUDA device id.\n"
     ]
    }
   ],
   "source": [
    "# llm_name = 'TheBloke/wizard-vicuna-13B-HF'\n",
    "llm_name = 'TheBloke/Wizard-Vicuna-13B-Uncensored-HF'\n",
    "llm = load_llm(llm_name, False, 'local', 0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_name, use_fast=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Stable Predictions for categorical prompt_style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_style = 'categorise'\n",
    "ensemble_size = 1\n",
    "edge_value = 'distribution'\n",
    "parse_style = 'categories_perplexity'\n",
    "relationship='budgetitem_to_indicator'\n",
    "effect_type = 'directly'\n",
    "\n",
    "prediction_generator = PredictionGenerator(None,\n",
    "                                            llm_name,\n",
    "                                            prompt_style,\n",
    "                                            ensemble_size,\n",
    "                                            edge_value,\n",
    "                                            parse_style,\n",
    "                                            relationship=relationship,\n",
    "                                            local_or_remote='local',\n",
    "                                            effect_type=effect_type)\n",
    "\n",
    "map_category_answer_b2i = { '1':'Local government spending on \"{budget_item}\" does {effect_type} affect \"{indicator}\"', '2':'Local government spending on \"{budget_item}\" does not {effect_type} affect \"{indicator}\"'}\n",
    "map_category_label_b2i = { '1':'Yes', '2':'No'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompts to test\n",
    "\n",
    "li_b2i = [\n",
    "    ('Children 5-19 public health programmes', 'Low birth weight of term babies','Yes'),\n",
    "    ('Children 5-19 public health programmes', 'Pupil absence','Yes'),\n",
    "\n",
    "    ('Education services', '5 or more A*-C grades at GCSE (inc english and maths)','Yes'),\n",
    "    ('Education services', 'Progression by 2 levels in maths between KS1 and KS2','Yes'),\n",
    "\n",
    "    ('Environmental and regulatory services', 'Municipal waste landfilled','Yes'),\n",
    "    ('Environmental and regulatory services', 'Waste collected per head','Yes'),\n",
    "\n",
    "    ('Children 5-19 public health programmes', 'Municipal waste landfilled','No'),\n",
    "    ('Children 5-19 public health programmes', 'Killed and seriously injured (KSI) casualties on England\\'s roads (historic data)','No'),\n",
    "\n",
    "    ('Education services', 'Municipal waste landfilled','No'),\n",
    "    ('Education services', 'Killed and seriously injured (KSI) casualties on England\\'s roads (historic data)','No'),\n",
    "\n",
    "    ('Environmental and regulatory services', '5 or more A*-C grades at GCSE (inc english and maths)','No'),\n",
    "    ('Environmental and regulatory services', 'Progression by 2 levels in maths between KS1 and KS2','No'),\n",
    "]\n",
    "\n",
    "li_budget_item = [ _tuple[0] for _tuple in li_b2i ]\n",
    "li_indicator = [ _tuple[1] for _tuple in li_b2i ]\n",
    "li_answer = [ _tuple[2] for _tuple in li_b2i ]\n",
    "\n",
    "\n",
    "li_test_prompts = [\n",
    "    \n",
    "    {'normal': f'Categories:\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}\\nWrite the category number that best answers whether local government spending on \"{{budget_item}}\" {{effect_type}} affects \"{{indicator}}\"?',\n",
    "    'reversed': f'Categories:\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}\\nWrite the category number that best answers whether local government spending on \"{{budget_item}}\" {{effect_type}} affects \"{{indicator}}\"?'\n",
    "    },\n",
    "\n",
    "    {'normal': f'Categories:\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}\\nWrite the number of the category that best answers whether local government spending on \"{{budget_item}}\" {{effect_type}} affects \"{{indicator}}\"?',\n",
    "    'reversed': f'Categories:\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}\\nWrite the number of the category that best answers whether local government spending on \"{{budget_item}}\" {{effect_type}} affects \"{{indicator}}\"?'\n",
    "    },\n",
    "\n",
    "    {'normal': f'Categories:\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}\\nWhat is the number of the category that best answers whether local government spending on \"{{budget_item}}\" {{effect_type}} affects \"{{indicator}}\"?',\n",
    "    'reversed': f'Categories:\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}\\nWhat is the number of the category that best answers whether local government spending on \"{{budget_item}}\" {{effect_type}} affects \"{{indicator}}\"?'\n",
    "    },\n",
    "\n",
    "    {'normal': f'Categories:\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}\\nWhat is the number of the category that answers the following question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?',\n",
    "    'reversed': f'Categories:\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}\\nWhat is the number of the category that answers the following question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?'\n",
    "    },\n",
    "\n",
    "    {'normal': f'Which of the following categories best answers the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Which of the following categories best answers the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}'\n",
    "     },\n",
    "\n",
    "    {'normal': f'Which of the following categories best answers the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\nCategories:\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Which of the following categories best answers the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\nCategories:\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"2\"]}'\n",
    "    },\n",
    "\n",
    "    {'normal': f'Which of the following categories best answers the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\nCategory 1) {map_category_answer_b2i[\"1\"]}\\nCategory 2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Which of the following categories best answers the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\nCategory 1) {map_category_answer_b2i[\"2\"]}\\nCategory 2) {map_category_answer_b2i[\"1\"]}'\n",
    "    },\n",
    "\n",
    "    {'normal': f'Category 1) {map_category_answer_b2i[\"1\"]}\\nCategory 2) {map_category_answer_b2i[\"2\"]}.\\n\\nQuestion: Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"\\n\\nChoose a category to answer the question.',\n",
    "    'reversed': f'Category 1) {map_category_answer_b2i[\"2\"]}\\nCategory 2) {map_category_answer_b2i[\"1\"]}.\\n\\nQuestion: Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"\\n\\nChoose a category to answer the question.'\n",
    "    },\n",
    "\n",
    "    {'normal': f'Select the category that answers the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Select the category that answers the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}'\n",
    "     },\n",
    "\n",
    "    {'normal': f'Write the number of the category that fits the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Write the number of the category that fits the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}'\n",
    "    },\n",
    "\n",
    "    {'normal': f'Write the number of the category that fits the question. Local government spending on \"{{budget_item}}\" {{effect_type}} affects \"{{indicator}}\"?\\n\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Write the number of the category that fits the question. Local government spending on \"{{budget_item}}\" {{effect_type}} affects \"{{indicator}}\"?\\n\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}'\n",
    "    },\n",
    "\n",
    "    {'normal': f'Write the number of the option that answers the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Write the number of the option that answers the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}'\n",
    "    },\n",
    "\n",
    "    {'normal': f'Write the number of the option that answers the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Write the number of the option that answers the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}'\n",
    "    },\n",
    "\n",
    "    {'normal': f'Write the number of the option that answers the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n\\nOptions\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Write the number of the option that answers the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n\\nOptions\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}'\n",
    "    },\n",
    "    \n",
    "    {'normal': f'Select the category number that answers the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Select the category number that answers the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}'\n",
    "     },\n",
    "\n",
    "    {'normal': f'Choose the category that answers the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Choose the category that answers the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}'\n",
    "     },\n",
    "\n",
    "    {'normal': f'Select the answer for the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Select the answer for the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}'\n",
    "     },\n",
    "\n",
    "    {'normal': f'Select the answer for the question.\\nQuestion: Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\nAnswers:\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Select the answer for the question.\\nQuestion: Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\nAnswers:\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}'\n",
    "     },\n",
    "\n",
    "    {'normal': f'Select the answer number that answers question.\\nQuestion: Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\nAnswers:\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Select the answer number that answers question.\\nQuestion: Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\nAnswers:\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}'\n",
    "     },\n",
    "\n",
    "    {'normal': f'Select the answer number that answers question.\\nQuestion: Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Select the answer number that answers question.\\nQuestion: Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}'\n",
    "     },\n",
    "\n",
    "    {'normal': f'Select the answer number that answers question.\\nQuestion: Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\nAnswer 1) {map_category_answer_b2i[\"1\"]}\\n Answer 2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Select the answer number that answers question.\\nQuestion: Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\nAnswer 1) {map_category_answer_b2i[\"2\"]}\\n Answer 2) {map_category_answer_b2i[\"1\"]}'\n",
    "     },\n",
    "\n",
    "    {'normal': f'Q: Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\nAnswers:\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}\\nSelect the correct answer for the question.',\n",
    "     'reversed': f'Q: Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\nAnswers:\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}\\nSelect the correct answer for the question.'\n",
    "     },\n",
    "\n",
    "    {'normal': f'Select the correct answer to the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Select the correct answer to the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}'\n",
    "     },\n",
    "\n",
    "    {'normal': f'Which answer is the correct answer to the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Which answer is the correct answer to the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}'\n",
    "     },\n",
    "\n",
    "    {'normal': f'Select the correct answer to the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n\\nAnswers:\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Select the correct answer to the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n\\nAnswers:\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}'\n",
    "     },\n",
    "\n",
    "    {'normal': f'Which answer is the correct answer to the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n\\nAnswers:\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Which answer is the correct answer to the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n\\nAnswers:\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}'\n",
    "     },\n",
    "\n",
    "\n",
    "    {'normal': f'Select the answer that answers the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Select the answer that answers the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}'\n",
    "     },\n",
    "\n",
    "    {'normal': f'Select the answer that answers the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\nAnswers:\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Select the answer that answers the question. Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\nAnswers:\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}'\n",
    "     },\n",
    "\n",
    "    {'normal': f'Is Answer 1 or Answer 2 the correct answer to the following Question: Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\nAnswers:\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Is Answer 1 or Answer 2 the correct answer to the following Question: Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\nAnswers:\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}'\n",
    "     },\n",
    "\n",
    "     {'normal': f'Is Answer 1 or Answer 2 the correct answer to the following Question: Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\nAnswer 1) {map_category_answer_b2i[\"1\"]}\\nAnswer 2) {map_category_answer_b2i[\"2\"]}',\n",
    "    'reversed': f'Is Answer 1 or Answer 2 the correct answer to the following Question: Does local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?\\nAnswer 1) {map_category_answer_b2i[\"2\"]}\\nAnswer 2) {map_category_answer_b2i[\"1\"]}'\n",
    "     },\n",
    "\n",
    "     {'normal': f'Answer 1) {map_category_answer_b2i[\"1\"]}\\nAnswer 2) {map_category_answer_b2i[\"2\"]}\\n\\nDoes local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?',\n",
    "    'reversed': f'Answer 1) {map_category_answer_b2i[\"2\"]}\\nAnswer 2) {map_category_answer_b2i[\"1\"]}\\n\\nDoes local government spending on \"{{budget_item}}\" {{effect_type}} affect \"{{indicator}}\"?'\n",
    "    },\n",
    "    \n",
    "    {'normal':f'Statement 1) {map_category_answer_b2i[\"1\"]}\\nStatement 2) {map_category_answer_b2i[\"2\"]}\\nWhich statement is True?',\n",
    "        'reversed':f'Statement 1) {map_category_answer_b2i[\"2\"]}\\nStatement 2) {map_category_answer_b2i[\"1\"]}\\nWhich statement is True?'\n",
    "    },\n",
    "\n",
    "    {'normal':f'Statement 1) {map_category_answer_b2i[\"1\"]}\\nStatement 2) {map_category_answer_b2i[\"2\"]}\\nI want you to select the True statement.',\n",
    "        'reversed':f'Statement 1) {map_category_answer_b2i[\"2\"]}\\nStatement 2) {map_category_answer_b2i[\"1\"]}\\nI want you to select the True statement.'\n",
    "    },\n",
    "\n",
    "    {'normal':f'Statement 1) {map_category_answer_b2i[\"1\"]}\\nStatement 2) {map_category_answer_b2i[\"2\"]}\\nIs Statement 1 or Statement 2 True?',\n",
    "        'reversed':f'Statement 1) {map_category_answer_b2i[\"2\"]}\\nStatement 2) {map_category_answer_b2i[\"1\"]}\\nIs Statement 1 or Statement 2 True?'\n",
    "    },\n",
    "\n",
    "    {'normal':f'Statement 1) {map_category_answer_b2i[\"1\"]}\\nStatement 2) {map_category_answer_b2i[\"2\"]}\\nWrite the number of the correct statement.',\n",
    "        'reversed':f'Statement 1) {map_category_answer_b2i[\"2\"]}\\nStatement 2) {map_category_answer_b2i[\"1\"]}\\nWrite the number of the correct statement.'\n",
    "    }\n",
    "    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the box below, we evaluate the output of all prompts on all budget item to indicator\n",
    "#### We effectively want to test that the normal and reverse version of the prompt give the same answer and that the outputs are robust to the order the options are presented in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/akann1warw1ck/miniconda3/envs/alanturing/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/akann1warw1ck/miniconda3/envs/alanturing/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/akann1warw1ck/miniconda3/envs/alanturing/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akann1warw1ck/miniconda3/envs/alanturing/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/akann1warw1ck/miniconda3/envs/alanturing/lib/libcudart.so'), PosixPath('/home/akann1warw1ck/miniconda3/envs/alanturing/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/akann1warw1ck/miniconda3/envs/alanturing/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/akann1warw1ck/miniconda3/envs/alanturing/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/akann1warw1ck/miniconda3/envs/alanturing/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akann1warw1ck/miniconda3/envs/alanturing/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/akann1warw1ck/miniconda3/envs/alanturing/lib/libcudart.so'), PosixPath('/home/akann1warw1ck/miniconda3/envs/alanturing/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "Using pad_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Normal\n",
      "   index  prob_correct\n",
      "0     10      0.566875\n",
      "1      9      0.558117\n",
      "2     24      0.547783\n",
      "3     18      0.546550\n",
      "4     11      0.534042\n",
      "5     14      0.532275\n",
      "6     17      0.532217\n",
      "7     27      0.528917\n",
      "8     13      0.528633\n",
      "9     20      0.526792\n",
      "\n",
      "Top 10 Reverse\n",
      "   index  prob_correct\n",
      "0     20      0.524958\n",
      "1      0      0.502000\n",
      "2      1      0.501083\n",
      "3      5      0.499933\n",
      "4     30      0.498208\n",
      "5      3      0.497892\n",
      "6     34      0.495817\n",
      "7      4      0.491992\n",
      "8     18      0.491817\n",
      "9      2      0.491250\n",
      "\n",
      "Top 10 Diffs\n",
      "   index      diff\n",
      "0      4  0.384308\n",
      "1     16  0.395683\n",
      "2     26  0.400392\n",
      "3      2  0.471092\n",
      "4     15  0.477458\n",
      "5     13  0.492908\n",
      "6      8  0.501433\n",
      "7     14  0.596367\n",
      "8      9  0.610233\n",
      "9     10  0.666617\n"
     ]
    }
   ],
   "source": [
    "from test_methods_helper import run_command\n",
    "debug = False\n",
    "\n",
    "li_format_dict = [ {'budget_item':b2i[0], 'indicator':b2i[1], 'effect_type':effect_type } for b2i in li_b2i ]\n",
    "li_related = [ b2i[2] for b2i in li_b2i ] \n",
    "\n",
    "if debug:\n",
    "    li_li_filledtemplate = [ [ test_prompt['normal'].format(**format_dict) ] for test_prompt in li_test_prompts[:2] for format_dict in li_format_dict ] \n",
    "    li_li_filledtemplate_reverse = [ [ test_prompt['reversed'].format(**format_dict) ] for test_prompt in li_test_prompts[:2] for format_dict in li_format_dict] \n",
    "    li_related_ = li_related*2 \n",
    "\n",
    "else:\n",
    "    li_li_filledtemplate = [ [ test_prompt['normal'].format(**format_dict) ] for test_prompt in li_test_prompts for format_dict in li_format_dict ] \n",
    "    li_li_filledtemplate_reverse = [ [ test_prompt['reversed'].format(**format_dict) ] for test_prompt in li_test_prompts for format_dict in li_format_dict ]\n",
    "    li_related_ = li_related*len(li_test_prompts) \n",
    "\n",
    "\n",
    "import multiprocessing as mp\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "# Create a queue to hold the results\n",
    "manager = mp.Manager()\n",
    "queue = mp.Queue()\n",
    "\n",
    "# Create two processes to run the commands\n",
    "p1 = mp.Process(target=run_command, args=(queue, prediction_generator, li_li_filledtemplate, llm_name, 0, 'normal' ))\n",
    "p2 = mp.Process(target=run_command, args=(queue, prediction_generator, li_li_filledtemplate_reverse, llm_name, 1, 'reverse' ))\n",
    "\n",
    "# Start the processes\n",
    "p1.start()\n",
    "p2.start()\n",
    "\n",
    "# Wait for the processes to finish\n",
    "p1.join()\n",
    "p2.join()\n",
    "\n",
    "# Get the results from the queue in the order they were put in\n",
    "results = []\n",
    "while not queue.empty():\n",
    "    results.append(queue.get())\n",
    "\n",
    "# Extract the results from the sorted list\n",
    "li_preds = [ d['li_preds'] for d in results if d['name'] == 'normal'  ][0]\n",
    "li_preds_reverse = [ d['li_preds'] for d in results if d['name'] == 'reverse'][0]\n",
    "\n",
    "# Evaluating the results\n",
    "## Goal is two produce two sets of rankings\n",
    "    ## One ranking is getting models which place the highest probability on the correct answer\n",
    "    ## One ranking is getting models which produce similar results for the normal and reversed prompts\n",
    "\n",
    "def get_prob_correctness(pred:dict, correct_answer:str) -> float:\n",
    "    if correct_answer == 'Yes':\n",
    "        return pred['Yes']\n",
    "    elif correct_answer == 'No':\n",
    "        return pred['No']\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def get_normal_reverse_diff( pred:str, pred_reverse:str ):\n",
    "    yes_diff = pred['Yes'] - pred_reverse['Yes']\n",
    "    return yes_diff\n",
    "\n",
    "li_prob_correct_normal = [ get_prob_correctness(pred, correct_answer) for pred, correct_answer in zip(li_preds, li_related_) ]\n",
    "li_prob_correct_reverse = [ get_prob_correctness(pred, correct_answer) for pred, correct_answer in zip(li_preds_reverse, li_related_) ]\n",
    "li_diffs = [ get_normal_reverse_diff(pred, pred_reverse) for pred, pred_reverse in zip(li_preds, li_preds_reverse) ]\n",
    "\n",
    "# Now we need to group the results by the prompt used and aggregate the reesults\n",
    "stride = len(li_format_dict)\n",
    "grouped_li_prob_correct_normal = [ li_prob_correct_normal[i:i+stride] for i in range(0, len(li_prob_correct_normal), stride ) ]\n",
    "grouped_li_prob_correct_reverse = [ li_prob_correct_reverse[i:i+stride] for i in range(0, len(li_prob_correct_reverse), stride ) ]\n",
    "grouped_li_diffs = [ li_diffs[i:i+stride] for i in range(0, len(li_diffs), stride ) ]\n",
    "\n",
    "avg_li_prob_correct_normal = [ sum(li)/len(li) for li in grouped_li_prob_correct_normal ]\n",
    "avg_li_prob_correct_reverse = [ sum(li)/len(li) for li in grouped_li_prob_correct_reverse ]\n",
    "avg_li_diffs = [ sum(li)/len(li) for li in grouped_li_diffs ]\n",
    "\n",
    "idx_top10_prob_correct_normal = sorted(range(len(avg_li_prob_correct_normal)), key=lambda i: avg_li_prob_correct_normal[i],reverse=True )[:10]\n",
    "idx_top10_prob_correct_reverse = sorted(range(len(avg_li_prob_correct_reverse)), key=lambda i: avg_li_prob_correct_reverse[i], reverse=True )[:10]\n",
    "idx_top10_diffs = sorted(range(len(avg_li_diffs)), key=lambda i: avg_li_diffs[i])[-10:]\n",
    "\n",
    "# Print a dataframe of top 10 normal with the prob correct answer as the first column\n",
    "df_top10_prob_correct_normal = pd.DataFrame( [ (idx, avg_li_prob_correct_normal[idx]) for idx in idx_top10_prob_correct_normal ], columns=['index', 'prob_correct'] )\n",
    "print(\"\\nTop 10 Normal\")\n",
    "print(df_top10_prob_correct_normal)\n",
    "\n",
    "# Print a dataframe of top 10 reverse with the prob correct answer as the first column\n",
    "df_top10_prob_correct_reverse = pd.DataFrame( [ (idx, avg_li_prob_correct_reverse[idx]) for idx in idx_top10_prob_correct_reverse ], columns=['index', 'prob_correct'] )\n",
    "print(\"\\nTop 10 Reverse\")\n",
    "print(df_top10_prob_correct_reverse)\n",
    "\n",
    "# Print a dataframe of top 10 diffs with the diff as the first column\n",
    "df_top10_diffs = pd.DataFrame( [ (idx, avg_li_diffs[idx]) for idx in idx_top10_diffs ], columns=['index', 'diff'] )\n",
    "print(\"\\nTop 10 Diffs\")\n",
    "print(df_top10_diffs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results above show all prompts essentially fail since the lowest diff is 0.75 which shows thmat the model is not robust to the order of the prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the continuation sentences output by the model for a given prompt\n",
    "- through this we can evaluate which prompts are appropriate e.g. the next token output should be 1 number of the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.llm_cache.clear()\n",
    "from prompt_engineering.utils_prompteng import (map_llmname_input_format, map_relationship_system_prompt)\n",
    "\n",
    "if 'llm' not in globals():\n",
    "    llm = load_llm(llm_name, False, 'local')\n",
    "\n",
    "def show_continuation( llm, prompt_idx=-1, method='normal', include_system_message=True, print_generations=True ):\n",
    "    \n",
    "    if method == 'normal':\n",
    "        li_filledtemplate = li_li_filledtemplate[prompt_idx]\n",
    "    elif method == 'reverse':\n",
    "        li_filledtemplate = li_li_filledtemplate_reverse[prompt_idx]\n",
    "    else:\n",
    "        raise ValueError('method must be normal or reverse')\n",
    "        \n",
    "    # Get the relationship and effect type\n",
    "    if include_system_message:\n",
    "        sm = (map_relationship_system_prompt[relationship][effect_type] + ' ' + map_relationship_system_prompt[relationship][prompt_style] ).replace('  ',' ').strip(' ')\n",
    "    else:\n",
    "        sm = None\n",
    "\n",
    "    li_prompt_adapted_to_lm = [ map_llmname_input_format(llm_name,\n",
    "                                    user_message = '\\n'+prompt, \n",
    "                                    system_message = sm )\n",
    "                                for prompt in li_filledtemplate ] #Added some base model formatting\n",
    "\n",
    "    llm.pipeline._forward_params  = {\n",
    "        # 'num_beams':3,\n",
    "        'num_return_sequences':1,\n",
    "        'early_stopping':True,\n",
    "        'max_new_tokens': 20,\n",
    "    }\n",
    "\n",
    "    outp = llm.predict(li_prompt_adapted_to_lm[0]+' ')\n",
    "\n",
    "    if print_generations:\n",
    "        print('\\t\\t========='+method.upper()+'=========')\n",
    "        print(li_prompt_adapted_to_lm[0])\n",
    "        print(outp)\n",
    "    return outp\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we are searching for the prompts that\n",
    "- have a continuation that starts with a number for both cases of normal and reverse\n",
    "- predict a different number for the normal and reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 26, 28, 34]\n"
     ]
    }
   ],
   "source": [
    "li_good_prompts_idx = []\n",
    "for idx in range(len(li_test_prompts)):\n",
    "    # print(f\"\\n\\nIDX={idx}\")\n",
    "\n",
    "    idx_ = idx*len(li_format_dict) # For each prompt we only compare the first b2i couple\n",
    "\n",
    "    pred_str = show_continuation(llm, prompt_idx=idx_, method='normal', print_generations=False)\n",
    "    pred_str_rev = show_continuation(llm, prompt_idx=idx_, method='reverse', print_generations=False)\n",
    "\n",
    "    # Perform a checks\n",
    "    bool_good_prompt = False\n",
    "    \n",
    "    if len(pred_str[0])>0 and len(pred_str_rev[0])>0 and pred_str[0].isdigit() and pred_str_rev[0].isdigit() and pred_str[0] != pred_str_rev[0]:\n",
    "        bool_good_prompt = True\n",
    "    \n",
    "    if bool_good_prompt:\n",
    "        li_good_prompts_idx.append(idx)\n",
    "\n",
    "print(li_good_prompts_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_good_prompts_idx = [0, 1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 26, 28, 34]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(li_good_prompts_idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation the performance of the models that satisfy the constraint that the foraward and reverse aggree\n",
    "- Now li_good_prompts_idx contains the indexes of the prompts that we want to use for performance comparison\n",
    "- We use the earlier experiment from before to re-evaluate the performance of the model on these prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/akann1warw1ck/miniconda3/envs/alanturing/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/akann1warw1ck/miniconda3/envs/alanturing/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/akann1warw1ck/miniconda3/envs/alanturing/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/akann1warw1ck/miniconda3/envs/alanturing/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/akann1warw1ck/miniconda3/envs/alanturing/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/akann1warw1ck/miniconda3/envs/alanturing/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akann1warw1ck/miniconda3/envs/alanturing/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/akann1warw1ck/miniconda3/envs/alanturing/lib/libcudart.so.11.0'), PosixPath('/home/akann1warw1ck/miniconda3/envs/alanturing/lib/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "/home/akann1warw1ck/miniconda3/envs/alanturing/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/akann1warw1ck/miniconda3/envs/alanturing/lib/libcudart.so'), PosixPath('/home/akann1warw1ck/miniconda3/envs/alanturing/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Normal\n",
      "   index  prob_correct\n",
      "0     34      0.599358\n",
      "1      2      0.574908\n",
      "2      0      0.563133\n",
      "3      1      0.548350\n",
      "4      3      0.537758\n",
      "5     23      0.522792\n",
      "6     28      0.518667\n",
      "7      9      0.511667\n",
      "8     14      0.509817\n",
      "9     10      0.509233\n",
      "\n",
      "Top 10 Reverse\n",
      "   index  prob_correct\n",
      "0      3      0.495708\n",
      "1     13      0.494092\n",
      "2     12      0.493250\n",
      "3      2      0.490942\n",
      "4     10      0.487108\n",
      "5     11      0.481725\n",
      "6      1      0.474842\n",
      "7     34      0.465167\n",
      "8      0      0.440967\n",
      "9     14      0.440533\n",
      "\n",
      "Top 10 Diffs\n",
      "   index      diff\n",
      "0      2  0.822283\n",
      "1     16  0.822900\n",
      "2      1  0.863475\n",
      "3      9  0.872758\n",
      "4      3  0.891800\n",
      "5     14  0.927733\n",
      "6     10  0.973975\n",
      "7     11  0.976033\n",
      "8     12  0.985600\n",
      "9     13  0.991917\n"
     ]
    }
   ],
   "source": [
    "from test_methods_helper import run_command\n",
    "debug = False\n",
    "\n",
    "li_format_dict = [ {'budget_item':b2i[0], 'indicator':b2i[1], 'effect_type':effect_type } for b2i in li_b2i ]\n",
    "li_related = [ b2i[2] for b2i in li_b2i ] \n",
    "\n",
    "if debug:\n",
    "    li_good_prompts_idx_ = li_good_prompts_idx[:2]\n",
    "    li_li_filledtemplate = [ [ li_test_prompts[idx]['normal'].format(**format_dict) ] for idx in li_good_prompts_idx_ for format_dict in li_format_dict ] \n",
    "    li_li_filledtemplate_reverse = [ [ li_test_prompts[idx]['reversed'].format(**format_dict) ] for idx in li_good_prompts_idx_  for format_dict in li_format_dict] \n",
    "    li_related_ = li_related*2 \n",
    "\n",
    "else:\n",
    "    li_li_filledtemplate = [ [ li_test_prompts[idx]['normal'].format(**format_dict) ] for idx in li_good_prompts_idx for format_dict in li_format_dict ] \n",
    "    li_li_filledtemplate_reverse = [ [ li_test_prompts[idx]['reversed'].format(**format_dict) ] for idx in li_good_prompts_idx for format_dict in li_format_dict ]\n",
    "    li_related_ = li_related*len(li_good_prompts_idx) \n",
    "\n",
    "\n",
    "import multiprocessing as mp\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "# Create a queue to hold the results\n",
    "manager = mp.Manager()\n",
    "queue = mp.Queue()\n",
    "\n",
    "# Create two processes to run the commands\n",
    "p1 = mp.Process(target=run_command, args=(queue, prediction_generator, li_li_filledtemplate, llm_name, 0, 'normal' ))\n",
    "p2 = mp.Process(target=run_command, args=(queue, prediction_generator, li_li_filledtemplate_reverse, llm_name, 1, 'reverse' ))\n",
    "\n",
    "# Start the processes\n",
    "p1.start()\n",
    "p2.start()\n",
    "\n",
    "# Wait for the processes to finish\n",
    "p1.join()\n",
    "p2.join()\n",
    "\n",
    "# Get the results from the queue in the order they were put in\n",
    "# put sentinel values in queue\n",
    "for _ in range(2):\n",
    "    queue.put(None)\n",
    "\n",
    "results = []\n",
    "while True:\n",
    "    data = queue.get()\n",
    "    if data is None:  # we met sentinel value, break the loop\n",
    "        break\n",
    "    results.append(data)\n",
    "\n",
    "\n",
    "# Extract the results from the sorted list\n",
    "li_preds = [ d['li_preds'] for d in results if d['name'] == 'normal'  ][0]\n",
    "li_preds_reverse = [ d['li_preds'] for d in results if d['name'] == 'reverse'][0]\n",
    "\n",
    "# Evaluating the results\n",
    "## Goal is two produce two sets of rankings\n",
    "    ## One ranking is getting models which place the highest probability on the correct answer\n",
    "    ## One ranking is getting models which produce similar results for the normal and reversed prompts\n",
    "\n",
    "def get_prob_correctness(pred:dict, correct_answer:str) -> float:\n",
    "    if correct_answer == 'Yes':\n",
    "        return pred['Yes']\n",
    "    elif correct_answer == 'No':\n",
    "        return pred['No']\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def get_normal_reverse_diff( pred:str, pred_reverse:str ):\n",
    "    yes_diff = pred['Yes'] - pred_reverse['Yes']\n",
    "    return yes_diff\n",
    "\n",
    "li_prob_correct_normal = [ get_prob_correctness(pred, correct_answer) for pred, correct_answer in zip(li_preds, li_related_) ]\n",
    "li_prob_correct_reverse = [ get_prob_correctness(pred, correct_answer) for pred, correct_answer in zip(li_preds_reverse, li_related_) ]\n",
    "li_diffs = [ get_normal_reverse_diff(pred, pred_reverse) for pred, pred_reverse in zip(li_preds, li_preds_reverse) ]\n",
    "\n",
    "# Now we need to group the results by the prompt used and aggregate the reesults\n",
    "stride = len(li_format_dict)\n",
    "grouped_li_prob_correct_normal = [ li_prob_correct_normal[i:i+stride] for i in range(0, len(li_prob_correct_normal), stride ) ]\n",
    "grouped_li_prob_correct_reverse = [ li_prob_correct_reverse[i:i+stride] for i in range(0, len(li_prob_correct_reverse), stride ) ]\n",
    "grouped_li_diffs = [ li_diffs[i:i+stride] for i in range(0, len(li_diffs), stride ) ]\n",
    "\n",
    "avg_li_prob_correct_normal = [ sum(li)/len(li) for li in grouped_li_prob_correct_normal ]\n",
    "avg_li_prob_correct_reverse = [ sum(li)/len(li) for li in grouped_li_prob_correct_reverse ]\n",
    "avg_li_diffs = [ sum(li)/len(li) for li in grouped_li_diffs ]\n",
    "\n",
    "idx_top10_prob_correct_normal = sorted(range(len(avg_li_prob_correct_normal)), key=lambda i: avg_li_prob_correct_normal[i],reverse=True )[:10]\n",
    "idx_top10_prob_correct_reverse = sorted(range(len(avg_li_prob_correct_reverse)), key=lambda i: avg_li_prob_correct_reverse[i], reverse=True )[:10]\n",
    "idx_top10_diffs = sorted(range(len(avg_li_diffs)), key=lambda i: avg_li_diffs[i])[-10:]\n",
    "\n",
    "# Print a dataframe of top 10 normal with the prob correct answer as the first column\n",
    "df_top10_prob_correct_normal = pd.DataFrame( [ (li_good_prompts_idx[idx], avg_li_prob_correct_normal[idx]) for idx in idx_top10_prob_correct_normal ], columns=['index', 'prob_correct'] )\n",
    "print(\"\\nTop 10 Normal\")\n",
    "print(df_top10_prob_correct_normal)\n",
    "\n",
    "# Print a dataframe of top 10 reverse with the prob correct answer as the first column\n",
    "df_top10_prob_correct_reverse = pd.DataFrame( [ (li_good_prompts_idx[idx], avg_li_prob_correct_reverse[idx]) for idx in idx_top10_prob_correct_reverse ], columns=['index', 'prob_correct'] )\n",
    "print(\"\\nTop 10 Reverse\")\n",
    "print(df_top10_prob_correct_reverse)\n",
    "\n",
    "# Print a dataframe of top 10 diffs with the diff as the first column\n",
    "df_top10_diffs = pd.DataFrame( [ (li_good_prompts_idx[idx], avg_li_diffs[idx]) for idx in idx_top10_diffs ], columns=['index', 'diff'] )\n",
    "print(\"\\nTop 10 Diffs\")\n",
    "print(df_top10_diffs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluatating Results \n",
    "- The diff results imply that none of the prompts are resistant to the reversal of the prompt.\n",
    "- However, our continuation results show that the prompts do corrrectly predict 1 or 2 next. Therefore we can assume that the likelihood predictor model has an error somewhere\n",
    "\n",
    "ERRORS TO CHECK\n",
    "- When I attach the number to the end, is the number the last token or is a final last token appended\n",
    "- The calculations used when calculating the perplexity or log-likehood  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 319, 1799, 9047, 13566, 29901]\n",
      "[1, 319, 1799, 9047, 13566, 29901, 29871]\n",
      "[1, 319, 1799, 9047, 13566, 29901, 29871, 29896]\n",
      "[1, 319, 1799, 9047, 13566, 29901, 29871, 29896, 29897]\n"
     ]
    }
   ],
   "source": [
    "# Testing the first point\n",
    "print(tokenizer.encode('ASSISTANT: 1) Local government spending on \"Environmental and regulatory services\" does directly affect \"Progression by 2 levels in maths between KS1 and KS2\" is the correct statement.'))\n",
    "print( tokenizer.encode(\"ASSISTANT:\") )\n",
    "print( tokenizer.encode(\"ASSISTANT: \") )\n",
    "print( tokenizer.encode(\"ASSISTANT: 1\") )\n",
    "print( tokenizer.encode(\"ASSISTANT: 1)\") )\n",
    "# Indeed we see the first point is not the problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To investigate if the likelihood of final token is being calculated properly we will break down the function\n",
    "\n",
    "GOING THROUGH THE FUNCTION WE IDENTIFY THE FOLLOWING POSSIBLE CAUSES FOR FAILURE\n",
    "- padding tokens on wrong side\n",
    "- add_start_token having un-wanted effects\n",
    "- the effect of not specifying max_length e.g. does it cut text during tokenization for long texts\n",
    "- args to tokenizer() e.g. add_special_tokens= False, padding=True, remember setting padding=left has to be done during model initiation\n",
    "\n",
    "\n",
    "- step by step through the for loop for calculating the perplexity\n",
    "\n",
    "CHANGES MADE\n",
    "- ensure padding is always on the left hand side w/ tokenizer.padding_size = 'left'. The default is right hand side\n",
    "- Change add_special_tokens=False to add_special_tokens=True\n",
    "- change Truncation to False permanently\n",
    "- change truncation to left side with tokenizer.truncation_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_engineering.utils_prompteng import map_llmname_input_format, map_relationship_system_prompt\n",
    "data_unfmtd = [ li_li_filledtemplate[0][0],\n",
    "    li_li_filledtemplate_reverse[0][0]]\n",
    "\n",
    "data_fmtd =  [\n",
    "                map_llmname_input_format(llm_name,\n",
    "                                        user_message = prompt, \n",
    "                                        system_message = (map_relationship_system_prompt[relationship][effect_type] + ' ' + map_relationship_system_prompt[relationship][prompt_style] ) )\n",
    "                                    for prompt in data_unfmtd ] \n",
    "data_fmtd_w_ans = [ s + ' ' +str(idx+1) for idx, s in enumerate(data_fmtd) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['USER: You are a socio-economic researcher tasked with answering a question about whether government spending on a \"government budget item\" directly affects a \"socio-economic/health indicator\". In the question the government budget item and socio-economic/health indicator will be presented within quotation marks.  Categories:\\n1) Local government spending on \"Children 5-19 public health programmes\" does directly affect \"Low birth weight of term babies\"\\n2) Local government spending on \"Children 5-19 public health programmes\" does not directly affect \"Low birth weight of term babies\"\\nWrite the category number that best answers whether local government spending on \"Children 5-19 public health programmes\" directly affects \"Low birth weight of term babies\"?\\nASSISTANT: 1',\n",
       " 'USER: You are a socio-economic researcher tasked with answering a question about whether government spending on a \"government budget item\" directly affects a \"socio-economic/health indicator\". In the question the government budget item and socio-economic/health indicator will be presented within quotation marks.  Categories:\\n1) Local government spending on \"Children 5-19 public health programmes\" does not directly affect \"Low birth weight of term babies\"\\n2) Local government spending on \"Children 5-19 public health programmes\" does directly affect \"Low birth weight of term babies\"\\nWrite the category number that best answers whether local government spending on \"Children 5-19 public health programmes\" directly affects \"Low birth weight of term babies\"?\\nASSISTANT: 2']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fmtd_w_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = load_llm(llm_name, local_or_remote='local')\n",
    "# data = [\n",
    "#     \"The number that comes after 1 is 2\",\n",
    "#     \"The number after 1 is 3\"\n",
    "# ]\n",
    "data = data_fmtd_w_ans\n",
    "\n",
    "model = llm.pipeline.model\n",
    "tokenizer = llm.pipeline.tokenizer\n",
    "batch_size = 2\n",
    "add_start_token = True\n",
    "max_length = None\n",
    "category_token_len = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def joint_probabilities_for_category(\n",
    "#     data, model, tokenizer, batch_size: int = 16, add_start_token: bool = True, max_length=None, category_token_len=1):\n",
    "\n",
    "\n",
    "# \"\"\"For a given prompt taking the style of \"Answer with the letter of the Category which best answers my question\", This function returns the joint probabilities for the category tokens in each posible answer,\n",
    "#     NOTE: by design the category responses must all be the same length, ideally 1 token length.\n",
    "\n",
    "#     NOTE: However the function is currently written to work on sequencs longer than 1 token, but this is not recommended.\n",
    "# \"\"\"\n",
    "import torch\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizerBase\n",
    "from torch.nn.functional import log_softmax\n",
    "\n",
    "assert isinstance(model, PreTrainedModel)\n",
    "assert isinstance(tokenizer, PreTrainedTokenizerBase)\n",
    "# assert category_token_len == 1, \"Currently only supports category tokens of length 1\"\n",
    "\n",
    "model = model\n",
    "tokenizer = tokenizer\n",
    "\n",
    "if tokenizer.pad_token is None and batch_size > 1:\n",
    "    existing_special_tokens = list(tokenizer.special_tokens_map_extended.values())\n",
    "    assert (\n",
    "        len(existing_special_tokens) > 0\n",
    "    ), \"If batch_size > 1, model must have at least one special token to use for padding. Please use a different model or set batch_size=1.\"\n",
    "    tokenizer.add_special_tokens({\"pad_token\": existing_special_tokens[0]})\n",
    "\n",
    "#NOTE: remove this section\n",
    "# if add_start_token and max_length:\n",
    "#     assert (\n",
    "#         tokenizer.bos_token is not None\n",
    "#     ), \"Input model must already have a BOS token if using add_start_token=True. Please use a different model, or set add_start_token=False\"\n",
    "#     max_tokenized_len = max_length - 1\n",
    "# else:\n",
    "#     max_tokenized_len = max_length\n",
    "\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.truncation_side = 'left'\n",
    "\n",
    "encodings = tokenizer(\n",
    "    data,\n",
    "    add_special_tokens=True,\n",
    "    padding=True,\n",
    "    truncation= False,\n",
    "    max_length=None,\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True,\n",
    "    \n",
    ").to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> USER: You are a socio-economic researcher tasked with answering a question about whether government spending on a \"government budget item\" directly affects a \"socio-economic/health indicator\". In the question the government budget item and socio-economic/health indicator will be presented within quotation marks.  Categories:\n",
      "1) Local government spending on \"Children 5-19 public health programmes\" does directly affect \"Low birth weight of term babies\"\n",
      "2) Local government spending on \"Children 5-19 public health programmes\" does not directly affect \"Low birth weight of term babies\"\n",
      "Write the category number that best answers whether local government spending on \"Children 5-19 public health programmes\" directly affects \"Low birth weight of term babies\"?\n",
      "ASSISTANT: 1\n",
      "<s> USER: You are a socio-economic researcher tasked with answering a question about whether government spending on a \"government budget item\" directly affects a \"socio-economic/health indicator\". In the question the government budget item and socio-economic/health indicator will be presented within quotation marks.  Categories:\n",
      "1) Local government spending on \"Children 5-19 public health programmes\" does not directly affect \"Low birth weight of term babies\"\n",
      "2) Local government spending on \"Children 5-19 public health programmes\" does directly affect \"Low birth weight of term babies\"\n",
      "Write the category number that best answers whether local government spending on \"Children 5-19 public health programmes\" directly affects \"Low birth weight of term babies\"?\n",
      "ASSISTANT: 2\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(encodings['input_ids'][0]))\n",
    "print(tokenizer.decode(encodings['input_ids'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_texts = encodings[\"input_ids\"]\n",
    "attn_masks = encodings[\"attention_mask\"]\n",
    "\n",
    "# Now we always have a start token\n",
    "# if add_start_token:\n",
    "#     assert torch.all(torch.ge(attn_masks.sum(1), 1)), \"Each input text must be at least one token long.\"\n",
    "# else:\n",
    "#     assert torch.all(\n",
    "#         torch.ge(attn_masks.sum(1), 2)\n",
    "#     ), \"When add_start_token=False, each input text must be at least two tokens long. Run with add_start_token=True if inputting strings of only one token, and remove all empty input strings.\"\n",
    "\n",
    "joint_probs = []\n",
    "\n",
    "#DEBUG # for start_index in range(0, len(encoded_texts), batch_size):\n",
    "start_index = 0\n",
    "# end_index = 1\n",
    "\n",
    "end_index = min(start_index + batch_size, len(encoded_texts))\n",
    "encoded_texts_batch = encoded_texts[start_index:end_index]\n",
    "attn_masks_batch = attn_masks[start_index:end_index]\n",
    "\n",
    "# if add_start_token:\n",
    "#     bos_tokens_tensor = torch.tensor([[tokenizer.bos_token_id]] * encoded_texts_batch.size(dim=0)).to(model.device)\n",
    "#     encoded_texts_batch = torch.cat([bos_tokens_tensor, encoded_texts_batch], dim=1)\n",
    "#     attn_masks_batch = torch.cat(\n",
    "#         [torch.ones(bos_tokens_tensor.size(), dtype=torch.int64).to(model.device), attn_masks_batch], dim=1\n",
    "#     )\n",
    "\n",
    "labels = encoded_texts_batch\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_logits = model(encoded_texts_batch, attention_mask=attn_masks_batch).logits\n",
    "\n",
    "shift_logits = out_logits[..., :-1, :]\n",
    "shift_labels = labels[..., 1:]\n",
    "shift_attention_mask_batch = attn_masks_batch[..., 1:]\n",
    "\n",
    "shift_logits = shift_logits[..., -category_token_len:, :]\n",
    "shift_labels = shift_labels[..., -category_token_len:]\n",
    "shift_attention_mask_batch = shift_attention_mask_batch[..., -category_token_len:]\n",
    "\n",
    "shift_logits = shift_logits.contiguous()\n",
    "shift_labels = shift_labels.contiguous()\n",
    "shift_attention_mask_batch = shift_attention_mask_batch.contiguous()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate log probabilities from logits\n",
    "log_probs  = log_softmax(shift_logits, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use gather to select the log probabilities for the actual tokens\n",
    "gathered_log_probs = log_probs.gather(-1, shift_labels.unsqueeze(-1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0418],\n",
       "        [-0.0599]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log_probs.max(dim=-1)\n",
    "gathered_log_probs\n",
    "# shift_attention_mask_batch\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "gathered_log_probs = gathered_log_probs * shift_attention_mask_batch\n",
    "\n",
    "# Sum the log probabilities for the actual tokens to get the joint log probability\n",
    "joint_log_prob_batch = gathered_log_probs.sum(dim=-1)\n",
    "\n",
    "joint_prob_batch = torch.exp(joint_log_prob_batch)\n",
    "\n",
    "# joint_probs += joint_prob_batch.tolist()\n",
    "\n",
    "# return joint_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9590, 0.9419], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_prob_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with ways to get stable Probabilities for cot categorical classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_style = 'cot_categorise'\n",
    "ensemble_size = 1\n",
    "edge_value = 'distribution'\n",
    "parse_style = 'categories_perplexity'\n",
    "relationship='budgetitem_to_indicator'\n",
    "effect_type = 'directly'\n",
    "\n",
    "prediction_generator = PredictionGenerator(llm,\n",
    "                                            llm_name,\n",
    "                                            prompt_style,\n",
    "                                            ensemble_size,\n",
    "                                            edge_value,\n",
    "                                            parse_style,\n",
    "                                            relationship=relationship,\n",
    "                                            local_or_remote='local',\n",
    "                                            effect_type=effect_type)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompts to test\n",
    "\n",
    "li_b2i = [\n",
    "    ('Children 5-19 public health programmes', 'Low birth weight of term babies','Yes'),\n",
    "    ('Children 5-19 public health programmes', 'Pupil absence','Yes'),\n",
    "\n",
    "    ('Education services', '5 or more A*-C grades at GCSE (inc english and maths)','Yes'),\n",
    "    ('Education services', 'Progression by 2 levels in maths between KS1 and KS2','Yes'),\n",
    "\n",
    "    ('Environmental and regulatory services', 'Municipal waste landfilled','Yes'),\n",
    "    ('Environmental and regulatory services', 'Waste collected per head','Yes'),\n",
    "\n",
    "    ('Children 5-19 public health programmes', 'Municipal waste landfilled','No'),\n",
    "    ('Children 5-19 public health programmes', 'Killed and seriously injured (KSI) casualties on England\\'s roads (historic data)','No'),\n",
    "\n",
    "    ('Education services', 'Municipal waste landfilled','No'),\n",
    "    ('Education services', 'Killed and seriously injured (KSI) casualties on England\\'s roads (historic data)','No'),\n",
    "\n",
    "    ('Environmental and regulatory services', '5 or more A*-C grades at GCSE (inc english and maths)','No'),\n",
    "    ('Environmental and regulatory services', 'Progression by 2 levels in maths between KS1 and KS2','No'),\n",
    "]\n",
    "\n",
    "li_budget_item = [ _tuple[0] for _tuple in li_b2i ]\n",
    "li_indicator = [ _tuple[1] for _tuple in li_b2i ]\n",
    "li_answer = [ _tuple[2] for _tuple in li_b2i ]\n",
    "\n",
    "map_category_answer_b2i = { '1':'Local government spending on \"{budget_item}\" does {effect_type} affect \"{indicator}\"', '2':'Local government spending on \"{budget_item}\" does not {effect_type} affect \"{indicator}\"'}\n",
    "map_category_label_b2i = { '1':'Yes', '2':'No'}\n",
    "\n",
    "li_test_prompts = [\n",
    "    \n",
    "    # {'normal': f'The Statement below expresses an opinion on whether government spending on \"{{budget_item}}\" {{effect_type}} affects \"{{indicator}}\". Classify the statement into one of the following categories by writing the selected category\\'s number.\\nCategory 1) {map_category_answer_b2i[\"1\"]}\\nCategory 2) {map_category_answer_b2i[\"2\"]}.\\nStatement: {\"{statement}\"}',\n",
    "    # 'reversed':f'The Statement below expresses an opinion on whether government spending on \"{{budget_item}}\" {{effect_type}} affects \"{{indicator}}\". Classify the statement into one of the following categories by writing the selected category\\'s number:\\nCategory 1) {map_category_answer_b2i[\"2\"]}\\nCategory 2) {map_category_answer_b2i[\"1\"]}.\\nStatement: {\"{statement}\"}'},\n",
    "\n",
    "\n",
    "    # {'normal': f'The Statement below expresses an opinion on whether government spending on \"{{budget_item}}\" {{effect_type}} affects \"{{indicator}}\". Classify the statement into one of the following categories by writing the selected category\\'s number.\\nCategories:\\n-1) {map_category_answer_b2i[\"1\"]}\\n-2) {map_category_answer_b2i[\"2\"]}.\\nStatement: {\"{statement}\"}',\n",
    "    # 'reversed':f'The Statement below expresses an opinion on whether government spending on \"{{budget_item}}\" {{effect_type}} affects \"{{indicator}}\". Classify the statement into one of the following categories by writing the selected category\\'s number:\\nCategories:\\n-1) {map_category_answer_b2i[\"2\"]}\\n-2) {map_category_answer_b2i[\"1\"]}.\\nStatement: {\"{statement}\"}'},\n",
    "\n",
    "    \n",
    "    # {'normal': f'Classify the statement into one of the following categories by only writing the selected category\\'s number.\\nCategory 1) {map_category_answer_b2i[\"1\"]}\\nCategory 2) {map_category_answer_b2i[\"2\"]}.\\nStatement: {\"{statement}\"}',\n",
    "    # 'reversed':f'Classify the statement into one of the following categories by only writing the selected category\\'s number:\\nCategory 1) {map_category_answer_b2i[\"2\"]}\\nCategory 2) {map_category_answer_b2i[\"1\"]}.\\nStatement: {\"{statement}\"}'},\n",
    "\n",
    "    # {'normal': f'Statement: {\"{statement}\"}\\n\\n Please classify the statement above into one of the following categories by only writing the selected category\\'s number.\\nCategory 1) {map_category_answer_b2i[\"1\"]}\\nCategory 2)      {map_category_answer_b2i[\"2\"]}.',\n",
    "    # 'reversed':f'Statement: {\"{statement}\"}\\n\\n Please classify the statement above into one of the following categories by only writing the selected category\\'s number:\\nCategory 1) {map_category_answer_b2i[\"2\"]}\\nCategory 2) {map_category_answer_b2i[\"1\"]}.'},\n",
    "\n",
    "\n",
    "    # {'normal': f'Statement: {\"{statement}\"}\\n\\n Please categorise the statement above by only writing the selected category\\'s number.\\nCategory 1) {map_category_answer_b2i[\"1\"]}\\nCategory 2) {map_category_answer_b2i[\"2\"]}.',\n",
    "    # 'reversed':f'Statement: {\"{statement}\"}\\n\\n Please categorise the statement above by only writing the selected category\\'s number:\\nCategory 1) {map_category_answer_b2i[\"2\"]}\\nCategory 2) {map_category_answer_b2i[\"1\"]}.'},\n",
    "\n",
    "    # {'normal': f'Statement: {\"{statement}\"}\\n\\nCategory 1) {map_category_answer_b2i[\"1\"]}\\t2) {map_category_answer_b2i[\"2\"]}\\n\\n Please categorise the statement into one of the categories',\n",
    "    #  'reversed':f'Statement: {\"{statement}\"}\\n\\nCategory 1) {map_category_answer_b2i[\"2\"]}\\t2) {map_category_answer_b2i[\"1\"]}\\n\\n Please categorise the statement into one of the categories'},\n",
    "     \n",
    "    # {'normal': f'Statement: {\"{statement}\"}\\n\\nCategory 1) {map_category_answer_b2i[\"1\"]}\\t2) {map_category_answer_b2i[\"2\"]}\\n\\nWhich category fits the statement?',\n",
    "    #  'reversed':f'Statement: {\"{statement}\"}\\n\\nCategory 1) {map_category_answer_b2i[\"2\"]}\\t2) {map_category_answer_b2i[\"1\"]}\\n\\nWhich category fits the statement?'},\n",
    "\n",
    "    # {'normal': f'Statement: {\"{statement}\"}\\n\\nCategories:\\n1) {map_category_answer_b2i[\"1\"]}\\t2) {map_category_answer_b2i[\"2\"]}\\n\\nWrite the number of the category that fits the statement',\n",
    "    #  'reversed':f'Statement: {\"{statement}\"}\\n\\nCategories:\\n1) {map_category_answer_b2i[\"2\"]}\\t2) {map_category_answer_b2i[\"1\"]}\\n\\nWrite the number of the category that fits the statement'},\n",
    "\n",
    "    # {'normal': f'Statement: {\"{statement}\"}\\n\\nCategories:\\n1) {map_category_answer_b2i[\"1\"]}\\t2) {map_category_answer_b2i[\"2\"]}\\n\\nWrite the number of the category that fits the statement.',\n",
    "    #  'reversed':f'Statement: {\"{statement}\"}\\n\\nCategories:\\n1) {map_category_answer_b2i[\"2\"]}\\t2) {map_category_answer_b2i[\"1\"]}\\n\\nWrite the number of the category that fits the statement.'},\n",
    "\n",
    "    {'normal': f'Write the number of the category that fits the following statement.\\nStatement: {\"{statement}\"}\\nCategories:\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}',\n",
    "     'reversed':f'Write the number of the category that fits the following statement.\\nStatement: {\"{statement}\"}\\nCategories:\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}'},\n",
    "\n",
    "    {'normal': f'Write only the number of the category that fits the following statement.\\nStatement: {\"{statement}\"}\\nCategories:\\n1) {map_category_answer_b2i[\"1\"]}\\n2) {map_category_answer_b2i[\"2\"]}',\n",
    "     'reversed':f'Write only the number of the category that fits the following statement.\\nStatement: {\"{statement}\"}\\nCategories:\\n1) {map_category_answer_b2i[\"2\"]}\\n2) {map_category_answer_b2i[\"1\"]}'},\n",
    "\n",
    "\n",
    "    # {'normal':f'Write \"1\" if the following statement implies {map_category_answer_b2i[\"1\"]} or write \"2\" if the following statement implies {map_category_answer_b2i[\"2\"]}.\\nStatement: {\"{statement}\"}',\n",
    "    # 'reversed':f'Write \"1\" if the following statement implies {map_category_answer_b2i[\"2\"]} or write \"2\" if the following statement implies {map_category_answer_b2i[\"1\"]}.\\nStatement: {\"{statement}\"}'},\n",
    "    \n",
    "    # {'normal':f'Write \"1\" if the following statement implies {map_category_answer_b2i[\"1\"]} is True or write \"2\" if the following statement implies it is False.\\nStatement: {\"{statement}\"}',\n",
    "    #  'reversed':f'Write \"1\" if the following statement implies {map_category_answer_b2i[\"2\"]} is True or write \"2\" if the following statement implies it is False.\\nStatement: {\"{statement}\"}'\n",
    "    #  }\n",
    "\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Intemediary statemetns which explain the language model's reasoning\n",
    "from prompt_engineering.utils_prompteng import map_llmname_input_format, map_relationship_system_prompt\n",
    "\n",
    "sm = 'You are a socio-economic researcher tasked with answering a question about whether government spending on a \"government budget item\" affects a \"socio-economic/health indicator\". In the question the government budget item and socio-economic/health indicator will be presented within quotation marks.'\n",
    "um_1= 'Using your expert knowledge, please provide a thorough, detailed and conclusive four sentence answer to the following question.'\n",
    "\n",
    "um_2 = 'To what extent, if any, does local government spending on \\\"{budget_item}\\\" {effect_type} affect \\\"{indicator}\\\"?'\n",
    "\n",
    "\n",
    "li_statements_prompts = [ map_llmname_input_format(llm_name, \n",
    "                                                   user_message= um_1 + ' ' + um_2.format(budget_item=budget_item, indicator=indicator, effect_type=effect_type),\n",
    "                                                   system_message= sm  \n",
    "                                                     ) for budget_item, indicator in zip(li_budget_item[:2], li_indicator[:2] ) ]\n",
    "\n",
    "llm.pipeline._forward_params['max_new_tokens'] = 200\n",
    "llm.pipeline._forward_params['early_stopping'] = True\n",
    "\n",
    "\n",
    "\n",
    "outputs = llm.generate( li_statements_prompts )\n",
    "li_statements = [ li_chatgen[0].text for li_chatgen in outputs.generations ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Statement: It is difficult to determine a direct correlation between local government spending on \"Mental Health\" and \"Satisfaction with Management of Roadworks\" without further context. Mental health services may address the needs of individuals affected by roadworks, but it is not clear if this would directly impact satisfaction with the management of roadworks. Additionally, other factors such as the quality of the roadworks themselves, the timeliness of completion, and the effectiveness of communication and compensation measures may also play a role in determining satisfaction.\\n\\nCategories:\\n1) Spending on \"Mental Health\" does directly affect \"Satisfaction with Management of Roadworks\"\\n2) Spending on \"Mental Health\" does not directly affect \"Satisfaction with Management of Roadworks\"\\n\\nWrite the number of the category that fits the statement']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akann1warw1ck/miniconda3/envs/alanturing/lib/python3.10/site-packages/transformers/generation/utils.py:1264: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "/home/akann1warw1ck/miniconda3/envs/alanturing/lib/python3.10/site-packages/transformers/generation/utils.py:1457: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2) Spending on \"Mental Health\" does not directly affect \"Satisfaction with Management of Roadworks\".'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from test_methods_helper import run_command\n",
    "debug = False\n",
    "\n",
    "# Get the statements explaining the language model's reasoning\n",
    "li_format_dict = [ {'budget_item':b2i[0], 'indicator':b2i[1], 'effect_type':effect_type, 'statement':statement } for b2i,statement in zip(li_b2i,li_statements) ]\n",
    "li_related = [ b2i[2] for b2i in li_b2i ] \n",
    "\n",
    "if debug:\n",
    "    li_li_filledtemplate = [ [ test_prompt['normal'].format(**format_dict) ] for test_prompt in li_test_prompts[:2] for format_dict in li_format_dict ] \n",
    "    li_li_filledtemplate_reverse = [ [ test_prompt['reversed'].format(**format_dict) ] for test_prompt in li_test_prompts[:2] for format_dict in li_format_dict] \n",
    "    li_related_ = li_related*2 \n",
    "\n",
    "else:\n",
    "    li_li_filledtemplate = [ [ test_prompt['normal'].format(**format_dict) ] for test_prompt in li_test_prompts for format_dict in li_format_dict ] \n",
    "    li_li_filledtemplate_reverse = [ [ test_prompt['reversed'].format(**format_dict) ] for test_prompt in li_test_prompts for format_dict in li_format_dict ]\n",
    "    li_related_ = li_related*len(li_test_prompts) \n",
    "\n",
    "\n",
    "import multiprocessing as mp\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "# Create a queue to hold the results\n",
    "manager = mp.Manager()\n",
    "queue = mp.Queue()\n",
    "\n",
    "# Create two processes to run the commands\n",
    "p1 = mp.Process(target=run_command, args=(queue, prediction_generator, li_li_filledtemplate, llm_name, 0, 'normal' ))\n",
    "p2 = mp.Process(target=run_command, args=(queue, prediction_generator, li_li_filledtemplate_reverse, llm_name, 1, 'reverse' ))\n",
    "\n",
    "# Start the processes\n",
    "p1.start()\n",
    "p2.start()\n",
    "\n",
    "# Wait for the processes to finish\n",
    "p1.join()\n",
    "p2.join()\n",
    "\n",
    "# Get the results from the queue in the order they were put in\n",
    "results = []\n",
    "while not queue.empty():\n",
    "    results.append(queue.get())\n",
    "\n",
    "# Extract the results from the sorted list\n",
    "li_preds = [ d['li_preds'] for d in results if d['name'] == 'normal'  ][0]\n",
    "li_preds_reverse = [ d['li_preds'] for d in results if d['name'] == 'reverse'][0]\n",
    "\n",
    "# Evaluating the results\n",
    "## Goal is two produce two sets of rankings\n",
    "    ## One ranking is getting models which place the highest probability on the correct answer\n",
    "    ## One ranking is getting models which produce similar results for the normal and reversed prompts\n",
    "\n",
    "def get_prob_correctness(pred:dict, correct_answer:str) -> float:\n",
    "    if correct_answer == 'Yes':\n",
    "        return pred['Yes']\n",
    "    elif correct_answer == 'No':\n",
    "        return pred['No']\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def get_normal_reverse_diff( pred:str, pred_reverse:str ):\n",
    "    yes_diff = pred['Yes'] - pred_reverse['Yes']\n",
    "    return yes_diff\n",
    "\n",
    "li_prob_correct_normal = [ get_prob_correctness(pred, correct_answer) for pred, correct_answer in zip(li_preds, li_related_) ]\n",
    "li_prob_correct_reverse = [ get_prob_correctness(pred, correct_answer) for pred, correct_answer in zip(li_preds_reverse, li_related_) ]\n",
    "li_diffs = [ get_normal_reverse_diff(pred, pred_reverse) for pred, pred_reverse in zip(li_preds, li_preds_reverse) ]\n",
    "\n",
    "# Now we need to group the results by the prompt used and aggregate the reesults\n",
    "stride = len(li_format_dict)\n",
    "grouped_li_prob_correct_normal = [ li_prob_correct_normal[i:i+stride] for i in range(0, len(li_prob_correct_normal), stride ) ]\n",
    "grouped_li_prob_correct_reverse = [ li_prob_correct_reverse[i:i+stride] for i in range(0, len(li_prob_correct_reverse), stride ) ]\n",
    "grouped_li_diffs = [ li_diffs[i:i+stride] for i in range(0, len(li_diffs), stride ) ]\n",
    "\n",
    "avg_li_prob_correct_normal = [ sum(li)/len(li) for li in grouped_li_prob_correct_normal ]\n",
    "avg_li_prob_correct_reverse = [ sum(li)/len(li) for li in grouped_li_prob_correct_reverse ]\n",
    "avg_li_diffs = [ sum(li)/len(li) for li in grouped_li_diffs ]\n",
    "\n",
    "idx_top10_prob_correct_normal = sorted(range(len(avg_li_prob_correct_normal)), key=lambda i: avg_li_prob_correct_normal[i],reverse=True )[:10]\n",
    "idx_top10_prob_correct_reverse = sorted(range(len(avg_li_prob_correct_reverse)), key=lambda i: avg_li_prob_correct_reverse[i], reverse=True )[:10]\n",
    "idx_top10_diffs = sorted(range(len(avg_li_diffs)), key=lambda i: avg_li_diffs[i])[-10:]\n",
    "\n",
    "# Print a dataframe of top 10 normal with the prob correct answer as the first column\n",
    "df_top10_prob_correct_normal = pd.DataFrame( [ (idx, avg_li_prob_correct_normal[idx]) for idx in idx_top10_prob_correct_normal ], columns=['index', 'prob_correct'] )\n",
    "print(\"\\nTop 10 Normal\")\n",
    "print(df_top10_prob_correct_normal)\n",
    "\n",
    "# Print a dataframe of top 10 reverse with the prob correct answer as the first column\n",
    "df_top10_prob_correct_reverse = pd.DataFrame( [ (idx, avg_li_prob_correct_reverse[idx]) for idx in idx_top10_prob_correct_reverse ], columns=['index', 'prob_correct'] )\n",
    "print(\"\\nTop 10 Reverse\")\n",
    "print(df_top10_prob_correct_reverse)\n",
    "\n",
    "# Print a dataframe of top 10 diffs with the diff as the first column\n",
    "df_top10_diffs = pd.DataFrame( [ (idx, avg_li_diffs[idx]) for idx in idx_top10_diffs ], columns=['index', 'diff'] )\n",
    "print(\"\\nTop 10 Diffs\")\n",
    "print(df_top10_diffs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.llm_cache.clear()\n",
    "from prompt_engineering.utils_prompteng import (map_llmname_input_format, map_relationship_system_prompt)\n",
    "\n",
    "if 'llm' not in globals():\n",
    "    llm = load_llm(llm_name, False, 'local')\n",
    "\n",
    "debug = False\n",
    "\n",
    "# Get the statements explaining the language model's reasoning\n",
    "li_format_dict = [ {'budget_item':b2i[0], 'indicator':b2i[1], 'effect_type':effect_type, 'statement':statement } for b2i,statement in zip(li_b2i,li_statements) ]\n",
    "li_related = [ b2i[2] for b2i in li_b2i ] \n",
    "\n",
    "if debug:\n",
    "    li_li_filledtemplate = [ [ test_prompt['normal'].format(**format_dict) ] for test_prompt in li_test_prompts[:2] for format_dict in li_format_dict ] \n",
    "    li_li_filledtemplate_reverse = [ [ test_prompt['reversed'].format(**format_dict) ] for test_prompt in li_test_prompts[:2] for format_dict in li_format_dict] \n",
    "    li_related_ = li_related*2 \n",
    "\n",
    "else:\n",
    "    li_li_filledtemplate = [ [ test_prompt['normal'].format(**format_dict) ] for test_prompt in li_test_prompts for format_dict in li_format_dict ] \n",
    "    li_li_filledtemplate_reverse = [ [ test_prompt['reversed'].format(**format_dict) ] for test_prompt in li_test_prompts for format_dict in li_format_dict ]\n",
    "    li_related_ = li_related*len(li_test_prompts) \n",
    "\n",
    "\n",
    "\n",
    "def show_continuation( llm, prompt_idx=-1, method='normal', include_system_message=True, print_generations=True ):\n",
    "    \n",
    "    if method == 'normal':\n",
    "        li_filledtemplate = li_li_filledtemplate[prompt_idx]\n",
    "    elif method == 'reverse':\n",
    "        li_filledtemplate = li_li_filledtemplate_reverse[prompt_idx]\n",
    "    else:\n",
    "        raise ValueError('method must be normal or reverse')\n",
    "        \n",
    "    # Get the relationship and effect type\n",
    "    if include_system_message:\n",
    "        sm = (map_relationship_system_prompt[relationship][effect_type] + ' ' + map_relationship_system_prompt[relationship][prompt_style] ).replace('  ',' ').strip(' ')\n",
    "    else:\n",
    "        sm = None\n",
    "\n",
    "    li_prompt_adapted_to_lm = [ map_llmname_input_format(llm_name,\n",
    "                                    user_message = prompt, \n",
    "                                    system_message = sm )\n",
    "                                for prompt in li_filledtemplate ] #Added some base model formatting\n",
    "\n",
    "    llm.pipeline._forward_params  = {\n",
    "        # 'num_beams':3,\n",
    "        'num_return_sequences':1,\n",
    "        'early_stopping':True,\n",
    "        'max_new_tokens': 2,\n",
    "    }\n",
    "\n",
    "    outp = llm.predict(li_prompt_adapted_to_lm[0]+' ')\n",
    "\n",
    "    if print_generations:\n",
    "        print('\\t\\t========='+method.upper()+'=========')\n",
    "        print(li_prompt_adapted_to_lm[0])\n",
    "        print(outp)\n",
    "    return outp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "idx_bool_good_prompts = []\n",
    "for idx in range(len(li_li_filledtemplate)):\n",
    "    pred = show_continuation(llm, idx, 'normal', include_system_message=False, print_generations=False)\n",
    "    pred_reverse = show_continuation(llm, idx, 'reverse', include_system_message=False, print_generations=False)\n",
    "\n",
    "    if pred[:1] != pred_reverse[:1] and pred[:1].isdigit() and pred_reverse[:1].isdigit():\n",
    "        idx_bool_good_prompts.append(idx)\n",
    "print(idx_bool_good_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print( len(li_format_dict) )\n",
    "print( len(li_li_filledtemplate) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++1++++\n",
      "\t\t=========NORMAL=========\n",
      "USER: Write the number of the category that fits the following statement.\n",
      "Statement: 1. According to research, there is a positive correlation between local government spending on \"Children 5-19 public health programmes\" and \"Pupil absence\". \n",
      "2. This suggests that investing in health and wellbeing programmes for children and young people can have a direct impact on reducing absenteeism in schools. \n",
      "3. However, other factors such as family income, parental employment, and school quality also play a role in pupil absence rates. \n",
      "4. Therefore, while there is a clear link between government spending on children's health programmes and pupil absence, further research is needed to fully understand the complex factors at play.\n",
      "Categories:\n",
      "1) Local government spending on \"Children 5-19 public health programmes\" does directly affect \"Pupil absence\"\n",
      "2) Local government spending on \"Children 5-19 public health programmes\" does not directly affect \"Pupil absence\"\n",
      "ASSISTANT: \n",
      "1.\n",
      "\t\t=========REVERSE=========\n",
      "USER: Write the number of the category that fits the following statement.\n",
      "Statement: 1. According to research, there is a positive correlation between local government spending on \"Children 5-19 public health programmes\" and \"Pupil absence\". \n",
      "2. This suggests that investing in health and wellbeing programmes for children and young people can have a direct impact on reducing absenteeism in schools. \n",
      "3. However, other factors such as family income, parental employment, and school quality also play a role in pupil absence rates. \n",
      "4. Therefore, while there is a clear link between government spending on children's health programmes and pupil absence, further research is needed to fully understand the complex factors at play.\n",
      "Categories:\n",
      "1) Local government spending on \"Children 5-19 public health programmes\" does not directly affect \"Pupil absence\"\n",
      "2) Local government spending on \"Children 5-19 public health programmes\" does directly affect \"Pupil absence\"\n",
      "ASSISTANT: \n",
      "2.\n",
      "\n",
      "\n",
      "\n",
      "++++2++++\n",
      "\t\t=========NORMAL=========\n",
      "USER: Write only the number of the category that fits the following statement.\n",
      "Statement: 1. According to research, there is a positive correlation between local government spending on \"Children 5-19 public health programmes\" and the \"Low birth weight of term babies\".\n",
      "2. This correlation suggests that increased government spending on public health programmes for children and adolescents may lead to better health outcomes for mothers and babies, ultimately reducing the incidence of low birth weight.\n",
      "3. However, other factors such as access to healthcare services, maternal age, and socioeconomic status also play a significant role in determining the health outcomes of mothers and babies.\n",
      "4. Therefore, while there is a clear relationship between government spending on public health programmes and low birth weight, further research is needed to fully understand the complex interplay of factors that contribute to this outcome.\n",
      "Categories:\n",
      "1) Local government spending on \"Children 5-19 public health programmes\" does directly affect \"Low birth weight of term babies\"\n",
      "2) Local government spending on \"Children 5-19 public health programmes\" does not directly affect \"Low birth weight of term babies\"\n",
      "ASSISTANT: \n",
      "1.\n",
      "\t\t=========REVERSE=========\n",
      "USER: Write only the number of the category that fits the following statement.\n",
      "Statement: 1. According to research, there is a positive correlation between local government spending on \"Children 5-19 public health programmes\" and the \"Low birth weight of term babies\".\n",
      "2. This correlation suggests that increased government spending on public health programmes for children and adolescents may lead to better health outcomes for mothers and babies, ultimately reducing the incidence of low birth weight.\n",
      "3. However, other factors such as access to healthcare services, maternal age, and socioeconomic status also play a significant role in determining the health outcomes of mothers and babies.\n",
      "4. Therefore, while there is a clear relationship between government spending on public health programmes and low birth weight, further research is needed to fully understand the complex interplay of factors that contribute to this outcome.\n",
      "Categories:\n",
      "1) Local government spending on \"Children 5-19 public health programmes\" does not directly affect \"Low birth weight of term babies\"\n",
      "2) Local government spending on \"Children 5-19 public health programmes\" does directly affect \"Low birth weight of term babies\"\n",
      "ASSISTANT: \n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "++++3++++\n",
      "\t\t=========NORMAL=========\n",
      "USER: Write only the number of the category that fits the following statement.\n",
      "Statement: 1. According to research, there is a positive correlation between local government spending on \"Children 5-19 public health programmes\" and \"Pupil absence\". \n",
      "2. This suggests that investing in health and wellbeing programmes for children and young people can have a direct impact on reducing absenteeism in schools. \n",
      "3. However, other factors such as family income, parental employment, and school quality also play a role in pupil absence rates. \n",
      "4. Therefore, while there is a clear link between government spending on children's health programmes and pupil absence, further research is needed to fully understand the complex factors at play.\n",
      "Categories:\n",
      "1) Local government spending on \"Children 5-19 public health programmes\" does directly affect \"Pupil absence\"\n",
      "2) Local government spending on \"Children 5-19 public health programmes\" does not directly affect \"Pupil absence\"\n",
      "ASSISTANT: \n",
      "1.\n",
      "\t\t=========REVERSE=========\n",
      "USER: Write only the number of the category that fits the following statement.\n",
      "Statement: 1. According to research, there is a positive correlation between local government spending on \"Children 5-19 public health programmes\" and \"Pupil absence\". \n",
      "2. This suggests that investing in health and wellbeing programmes for children and young people can have a direct impact on reducing absenteeism in schools. \n",
      "3. However, other factors such as family income, parental employment, and school quality also play a role in pupil absence rates. \n",
      "4. Therefore, while there is a clear link between government spending on children's health programmes and pupil absence, further research is needed to fully understand the complex factors at play.\n",
      "Categories:\n",
      "1) Local government spending on \"Children 5-19 public health programmes\" does not directly affect \"Pupil absence\"\n",
      "2) Local government spending on \"Children 5-19 public health programmes\" does directly affect \"Pupil absence\"\n",
      "ASSISTANT: \n",
      "2\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in idx_bool_good_prompts:\n",
    "    print('++++'+str(idx)+'++++')\n",
    "    show_continuation(llm, idx, 'normal', include_system_message=False, print_generations=True)\n",
    "    show_continuation(llm, idx, 'reverse', include_system_message=False, print_generations=True)\n",
    "    print( '\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alanturing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
